{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JTqHs3_m3Hf0",
        "outputId": "0ea4932c-cd20-412e-a70b-b9fa5a5eb867"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentences after preprocessing: ['test data test sentence', 'contains html tags numbers like fifty special characters', 'let us test contractions']\n",
            "Extracted Bigrams: []\n",
            "Tokenized Sentences: [['test', 'data', 'test', 'sentence'], ['contains', 'html', 'tags', 'numbers', 'like', 'fifty', 'special', 'characters'], ['let', 'us', 'test', 'contractions']]\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import nltk\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from word2number import w2n\n",
        "from contractions import contractions_dict\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Remove HTML tags\n",
        "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
        "\n",
        "    # Remove extra whitespaces\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    # Sentence boundary detection\n",
        "    sentences = sent_tokenize(text)\n",
        "\n",
        "    # Expand contractions\n",
        "    def expand_contractions(text, contractions_dict=contractions_dict):\n",
        "        contractions_pattern = re.compile('({})'.format('|'.join(contractions_dict.keys())),\n",
        "                                          flags=re.IGNORECASE|re.DOTALL)\n",
        "        def expand_match(contraction):\n",
        "            match = contraction.group(0)\n",
        "            expanded_contraction = contractions_dict.get(match)\\\n",
        "                if contractions_dict.get(match)\\\n",
        "                else contractions_dict.get(match.lower())\n",
        "            return expanded_contraction\n",
        "\n",
        "        expanded_text = contractions_pattern.sub(expand_match, text)\n",
        "        expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
        "        return expanded_text\n",
        "\n",
        "    sentences = [expand_contractions(sentence) for sentence in sentences]\n",
        "\n",
        "    # Remove special characters\n",
        "    sentences = [re.sub(r'[^\\w\\s]', '', sentence) for sentence in sentences]\n",
        "\n",
        "    # Lowercase all texts\n",
        "    sentences = [sentence.lower() for sentence in sentences]\n",
        "\n",
        "    # Convert number words to numeric form\n",
        "    sentences = [w2n.word_to_num(sentence) if sentence.isdigit() else sentence for sentence in sentences]\n",
        "\n",
        "    # Remove numbers\n",
        "    sentences = [re.sub(r'\\d+', '', sentence) for sentence in sentences]\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    sentences = [' '.join([word for word in sentence.split() if word not in stop_words]) for sentence in sentences]\n",
        "\n",
        "    # Phrase extraction (Example using NLTK's collocations)\n",
        "    bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
        "    finder = nltk.collocations.BigramCollocationFinder.from_words(word_tokenize(' '.join(sentences)))\n",
        "    finder.apply_freq_filter(3)\n",
        "    bigrams = finder.nbest(bigram_measures.pmi, 10)\n",
        "\n",
        "    # Tokenization\n",
        "    tokens = [word_tokenize(sentence) for sentence in sentences]\n",
        "\n",
        "    return {\n",
        "        'sentences': sentences,\n",
        "        'bigrams': bigrams,\n",
        "        'tokens': tokens\n",
        "    }\n",
        "\n",
        "# Dummy data\n",
        "dummy_data = \"\"\"\n",
        "<html>\n",
        "    <head><title>Test Data</title></head>\n",
        "    <body>\n",
        "        This is a test sentence. It contains some HTML tags, numbers like fifty and 123, and special characters! Let's test contractions too: won't, haven't, they're.\n",
        "    </body>\n",
        "</html>\n",
        "\"\"\"\n",
        "\n",
        "# Preprocess the dummy data\n",
        "preprocessed_data = preprocess_text(dummy_data)\n",
        "\n",
        "print(\"Sentences after preprocessing:\", preprocessed_data['sentences'])\n",
        "print(\"Extracted Bigrams:\", preprocessed_data['bigrams'])\n",
        "print(\"Tokenized Sentences:\", preprocessed_data['tokens'])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install contractions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J7GjlMOY3LAX",
        "outputId": "9df158a7-611b-410e-8fed-b78df74df6a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting contractions\n",
            "  Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
            "Collecting textsearch>=0.0.21 (from contractions)\n",
            "  Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
            "Collecting anyascii (from textsearch>=0.0.21->contractions)\n",
            "  Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyahocorasick (from textsearch>=0.0.21->contractions)\n",
            "  Downloading pyahocorasick-2.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.7/110.7 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.1.0 textsearch-0.0.24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from word2number import w2n\n",
        "from contractions import contractions_dict\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Initial Text\n",
        "    print(f\"Original Text:\\n{text}\\n\")\n",
        "\n",
        "    # Remove HTML tags\n",
        "    text_no_html = BeautifulSoup(text, \"html.parser\").get_text()\n",
        "    print(f\"Text after removing HTML tags:\\n{text_no_html}\\n\")\n",
        "\n",
        "    # Remove extra whitespaces\n",
        "    text_no_whitespace = re.sub(r'\\s+', ' ', text_no_html).strip()\n",
        "    print(f\"Text after removing extra whitespaces:\\n{text_no_whitespace}\\n\")\n",
        "\n",
        "    # Sentence boundary detection\n",
        "    sentences = sent_tokenize(text_no_whitespace)\n",
        "    print(f\"Sentences after boundary detection:\\n{sentences}\\n\")\n",
        "\n",
        "    # Expand contractions\n",
        "    def expand_contractions(text, contractions_dict=contractions_dict):\n",
        "        contractions_pattern = re.compile('({})'.format('|'.join(contractions_dict.keys())),\n",
        "                                          flags=re.IGNORECASE|re.DOTALL)\n",
        "        def expand_match(contraction):\n",
        "            match = contraction.group(0)\n",
        "            expanded_contraction = contractions_dict.get(match)\\\n",
        "                if contractions_dict.get(match)\\\n",
        "                else contractions_dict.get(match.lower())\n",
        "            return expanded_contraction\n",
        "\n",
        "        expanded_text = contractions_pattern.sub(expand_match, text)\n",
        "        expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
        "        return expanded_text\n",
        "\n",
        "    sentences_expanded = [expand_contractions(sentence) for sentence in sentences]\n",
        "    print(f\"Sentences after expanding contractions:\\n{sentences_expanded}\\n\")\n",
        "\n",
        "    # Remove special characters\n",
        "    sentences_no_special_chars = [re.sub(r'[^\\w\\s]', '', sentence) for sentence in sentences_expanded]\n",
        "    print(f\"Sentences after removing special characters:\\n{sentences_no_special_chars}\\n\")\n",
        "\n",
        "    # Lowercase all texts\n",
        "    sentences_lowercase = [sentence.lower() for sentence in sentences_no_special_chars]\n",
        "    print(f\"Sentences after converting to lowercase:\\n{sentences_lowercase}\\n\")\n",
        "\n",
        "    # Convert number words to numeric form\n",
        "    def convert_number_words(sentence):\n",
        "        words = sentence.split()\n",
        "        new_words = []\n",
        "        for word in words:\n",
        "            try:\n",
        "                # Convert number words to numbers if possible\n",
        "                num = w2n.word_to_num(word)\n",
        "                new_words.append(str(num))\n",
        "            except ValueError:\n",
        "                # Keep the word if it's not a number word\n",
        "                new_words.append(word)\n",
        "        return ' '.join(new_words)\n",
        "\n",
        "    sentences_numbers = [convert_number_words(sentence) for sentence in sentences_lowercase]\n",
        "    print(f\"Sentences after converting number words to numeric form:\\n{sentences_numbers}\\n\")\n",
        "\n",
        "    # Remove numbers\n",
        "    sentences_no_numbers = [re.sub(r'\\d+', '', sentence) for sentence in sentences_numbers]\n",
        "    print(f\"Sentences after removing numbers:\\n{sentences_no_numbers}\\n\")\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    sentences_no_stopwords = [' '.join([word for word in sentence.split() if word not in stop_words]) for sentence in sentences_no_numbers]\n",
        "    print(f\"Sentences after removing stopwords:\\n{sentences_no_stopwords}\\n\")\n",
        "\n",
        "    # Phrase extraction (Example using NLTK's collocations)\n",
        "    bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
        "    finder = nltk.collocations.BigramCollocationFinder.from_words(word_tokenize(' '.join(sentences_no_stopwords)))\n",
        "    finder.apply_freq_filter(3)\n",
        "    bigrams = finder.nbest(bigram_measures.pmi, 10)\n",
        "    print(f\"Extracted Bigrams:\\n{bigrams}\\n\")\n",
        "\n",
        "    # Tokenization\n",
        "    tokens = [word_tokenize(sentence) for sentence in sentences_no_stopwords]\n",
        "    print(f\"Tokenized Sentences:\\n{tokens}\\n\")\n",
        "\n",
        "    return {\n",
        "        'sentences': sentences_no_stopwords,\n",
        "        'bigrams': bigrams,\n",
        "        'tokens': tokens\n",
        "    }\n",
        "\n",
        "# Dummy data\n",
        "dummy_data = \"\"\"<html>\n",
        "    <head>\n",
        "        <title>Sample HTML Document</title>\n",
        "    </head>\n",
        "    <body>\n",
        "        <h1>Welcome to NLP Processing</h1>\n",
        "        <p>This is a sample document with various HTML    elements. It includes numbers like one hundred and twenty-three, and also special characters: @, #, $. Furthermore, let's test contractions such as won't, can't, and doesn't. Lastly, check out some common phrases: data science, machine learning, and artificial intelligence!</p>\n",
        "        <p>Another paragraph with different content. Numbers such as 2024, 15.6, and twenty-five should be handled. Also, more HTML tags and additional special characters like & and * should be removed.</p>\n",
        "    </body>\n",
        "</html>\"\"\"\n",
        "\n",
        "\n",
        "# Preprocess the dummy data\n",
        "preprocessed_data = preprocess_text(dummy_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0rESdNt93VNW",
        "outputId": "4ed25ee4-8f17-45e6-bad8-3d135f5d1424"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text:\n",
            "<html>\n",
            "    <head>\n",
            "        <title>Sample HTML Document</title>\n",
            "    </head>\n",
            "    <body>\n",
            "        <h1>Welcome to NLP Processing</h1>\n",
            "        <p>This is a sample document with various HTML    elements. It includes numbers like one hundred and twenty-three, and also special characters: @, #, $. Furthermore, let's test contractions such as won't, can't, and doesn't. Lastly, check out some common phrases: data science, machine learning, and artificial intelligence!</p>\n",
            "        <p>Another paragraph with different content. Numbers such as 2024, 15.6, and twenty-five should be handled. Also, more HTML tags and additional special characters like & and * should be removed.</p>\n",
            "    </body>\n",
            "</html>\n",
            "\n",
            "Text after removing HTML tags:\n",
            "\n",
            "\n",
            "Sample HTML Document\n",
            "\n",
            "\n",
            "Welcome to NLP Processing\n",
            "This is a sample document with various HTML    elements. It includes numbers like one hundred and twenty-three, and also special characters: @, #, $. Furthermore, let's test contractions such as won't, can't, and doesn't. Lastly, check out some common phrases: data science, machine learning, and artificial intelligence!\n",
            "Another paragraph with different content. Numbers such as 2024, 15.6, and twenty-five should be handled. Also, more HTML tags and additional special characters like & and * should be removed.\n",
            "\n",
            "\n",
            "\n",
            "Text after removing extra whitespaces:\n",
            "Sample HTML Document Welcome to NLP Processing This is a sample document with various HTML elements. It includes numbers like one hundred and twenty-three, and also special characters: @, #, $. Furthermore, let's test contractions such as won't, can't, and doesn't. Lastly, check out some common phrases: data science, machine learning, and artificial intelligence! Another paragraph with different content. Numbers such as 2024, 15.6, and twenty-five should be handled. Also, more HTML tags and additional special characters like & and * should be removed.\n",
            "\n",
            "Sentences after boundary detection:\n",
            "['Sample HTML Document Welcome to NLP Processing This is a sample document with various HTML elements.', 'It includes numbers like one hundred and twenty-three, and also special characters: @, #, $.', \"Furthermore, let's test contractions such as won't, can't, and doesn't.\", 'Lastly, check out some common phrases: data science, machine learning, and artificial intelligence!', 'Another paragraph with different content.', 'Numbers such as 2024, 15.6, and twenty-five should be handled.', 'Also, more HTML tags and additional special characters like & and * should be removed.']\n",
            "\n",
            "Sentences after expanding contractions:\n",
            "['Sample HTML Document Welcome to NLP Processing This is a sample document with various HTML elements.', 'It includes numbers like one hundred and twenty-three, and also special characters: @, #, $.', 'Furthermore, let us test contractions such as will not, cannot, and does not.', 'Lastly, check out some common phrases: data science, machine learning, and artificial intelligence!', 'Another paragraph with different content.', 'Numbers such as 2024, 15.6, and twenty-five should be handled.', 'Also, more HTML tags and additional special characters like & and * should be removed.']\n",
            "\n",
            "Sentences after removing special characters:\n",
            "['Sample HTML Document Welcome to NLP Processing This is a sample document with various HTML elements', 'It includes numbers like one hundred and twentythree and also special characters   ', 'Furthermore let us test contractions such as will not cannot and does not', 'Lastly check out some common phrases data science machine learning and artificial intelligence', 'Another paragraph with different content', 'Numbers such as 2024 156 and twentyfive should be handled', 'Also more HTML tags and additional special characters like  and  should be removed']\n",
            "\n",
            "Sentences after converting to lowercase:\n",
            "['sample html document welcome to nlp processing this is a sample document with various html elements', 'it includes numbers like one hundred and twentythree and also special characters   ', 'furthermore let us test contractions such as will not cannot and does not', 'lastly check out some common phrases data science machine learning and artificial intelligence', 'another paragraph with different content', 'numbers such as 2024 156 and twentyfive should be handled', 'also more html tags and additional special characters like  and  should be removed']\n",
            "\n",
            "Sentences after converting number words to numeric form:\n",
            "['sample html document welcome to nlp processing this is a sample document with various html elements', 'it includes numbers like 1 100 and twentythree and also special characters', 'furthermore let us test contractions such as will not cannot and does not', 'lastly check out some common phrases data science machine learning and artificial intelligence', 'another paragraph with different content', 'numbers such as 2024 156 and twentyfive should be handled', 'also more html tags and additional special characters like and should be removed']\n",
            "\n",
            "Sentences after removing numbers:\n",
            "['sample html document welcome to nlp processing this is a sample document with various html elements', 'it includes numbers like   and twentythree and also special characters', 'furthermore let us test contractions such as will not cannot and does not', 'lastly check out some common phrases data science machine learning and artificial intelligence', 'another paragraph with different content', 'numbers such as   and twentyfive should be handled', 'also more html tags and additional special characters like and should be removed']\n",
            "\n",
            "Sentences after removing stopwords:\n",
            "['sample html document welcome nlp processing sample document various html elements', 'includes numbers like twentythree also special characters', 'furthermore let us test contractions cannot', 'lastly check common phrases data science machine learning artificial intelligence', 'another paragraph different content', 'numbers twentyfive handled', 'also html tags additional special characters like removed']\n",
            "\n",
            "Extracted Bigrams:\n",
            "[]\n",
            "\n",
            "Tokenized Sentences:\n",
            "[['sample', 'html', 'document', 'welcome', 'nlp', 'processing', 'sample', 'document', 'various', 'html', 'elements'], ['includes', 'numbers', 'like', 'twentythree', 'also', 'special', 'characters'], ['furthermore', 'let', 'us', 'test', 'contractions', 'can', 'not'], ['lastly', 'check', 'common', 'phrases', 'data', 'science', 'machine', 'learning', 'artificial', 'intelligence'], ['another', 'paragraph', 'different', 'content'], ['numbers', 'twentyfive', 'handled'], ['also', 'html', 'tags', 'additional', 'special', 'characters', 'like', 'removed']]\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk import pos_tag, RegexpParser\n",
        "from word2number import w2n\n",
        "from contractions import contractions_dict\n",
        "\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "\n",
        "def preprocess_text(text):\n",
        "\n",
        "    print(f\"Original Text:\\n{text}\\n\")\n",
        "\n",
        "\n",
        "    text_no_html = BeautifulSoup(text, \"html.parser\").get_text()\n",
        "    print(f\"Text after removing HTML tags:\\n{text_no_html}\\n\")\n",
        "\n",
        "\n",
        "    text_no_whitespace = re.sub(r'\\s+', ' ', text_no_html).strip()\n",
        "    print(f\"Text after removing extra whitespaces:\\n{text_no_whitespace}\\n\")\n",
        "\n",
        "\n",
        "    sentences = sent_tokenize(text_no_whitespace)\n",
        "    print(f\"Sentences after boundary detection:\\n{sentences}\\n\")\n",
        "\n",
        "\n",
        "    def expand_contractions(text, contractions_dict=contractions_dict):\n",
        "        contractions_pattern = re.compile('({})'.format('|'.join(contractions_dict.keys())),\n",
        "                                          flags=re.IGNORECASE|re.DOTALL)\n",
        "        def expand_match(contraction):\n",
        "            match = contraction.group(0)\n",
        "            expanded_contraction = contractions_dict.get(match)\\\n",
        "                if contractions_dict.get(match)\\\n",
        "                else contractions_dict.get(match.lower())\n",
        "            return expanded_contraction\n",
        "\n",
        "        expanded_text = contractions_pattern.sub(expand_match, text)\n",
        "        expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
        "        return expanded_text\n",
        "\n",
        "    sentences_expanded = [expand_contractions(sentence) for sentence in sentences]\n",
        "    print(f\"Sentences after expanding contractions:\\n{sentences_expanded}\\n\")\n",
        "\n",
        "\n",
        "    sentences_no_special_chars = [re.sub(r'[^\\w\\s]', '', sentence) for sentence in sentences_expanded]\n",
        "    print(f\"Sentences after removing special characters:\\n{sentences_no_special_chars}\\n\")\n",
        "\n",
        "\n",
        "    sentences_lowercase = [sentence.lower() for sentence in sentences_no_special_chars]\n",
        "    print(f\"Sentences after converting to lowercase:\\n{sentences_lowercase}\\n\")\n",
        "\n",
        "\n",
        "    def convert_number_words(sentence):\n",
        "        words = sentence.split()\n",
        "        new_words = []\n",
        "        for word in words:\n",
        "            try:\n",
        "\n",
        "                num = w2n.word_to_num(word)\n",
        "                new_words.append(str(num))\n",
        "            except ValueError:\n",
        "\n",
        "                new_words.append(word)\n",
        "        return ' '.join(new_words)\n",
        "\n",
        "    sentences_numbers = [convert_number_words(sentence) for sentence in sentences_lowercase]\n",
        "    print(f\"Sentences after converting number words to numeric form:\\n{sentences_numbers}\\n\")\n",
        "\n",
        "\n",
        "    sentences_no_numbers = [re.sub(r'\\d+', '', sentence) for sentence in sentences_numbers]\n",
        "    print(f\"Sentences after removing numbers:\\n{sentences_no_numbers}\\n\")\n",
        "\n",
        "\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    sentences_no_stopwords = [' '.join([word for word in sentence.split() if word not in stop_words]) for sentence in sentences_no_numbers]\n",
        "    print(f\"Sentences after removing stopwords:\\n{sentences_no_stopwords}\\n\")\n",
        "\n",
        "    bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
        "    finder = nltk.collocations.BigramCollocationFinder.from_words(word_tokenize(' '.join(sentences_no_stopwords)))\n",
        "    finder.apply_freq_filter(3)\n",
        "    bigrams = finder.nbest(bigram_measures.pmi, 10)\n",
        "    print(f\"Extracted Bigrams:\\n{bigrams}\\n\")\n",
        "\n",
        "\n",
        "    def extract_phrases(text):\n",
        "        tokens = word_tokenize(text)\n",
        "        tagged = pos_tag(tokens)\n",
        "        chunk_grammar = \"NP: {<DT>?<JJ>*<NN>+}\"\n",
        "        chunk_parser = RegexpParser(chunk_grammar)\n",
        "        tree = chunk_parser.parse(tagged)\n",
        "        phrases = set()\n",
        "        for subtree in tree.subtrees(filter=lambda t: t.label() == 'NP'):\n",
        "            phrase = \" \".join(word for word, tag in subtree.leaves())\n",
        "            phrases.add(phrase)\n",
        "        return list(phrases)\n",
        "\n",
        "    all_sentences_text = ' '.join(sentences_no_stopwords)\n",
        "    phrases = extract_phrases(all_sentences_text)\n",
        "    print(f\"Extracted Phrases:\\n{phrases}\\n\")\n",
        "\n",
        "\n",
        "    tokens = [word_tokenize(sentence) for sentence in sentences_no_stopwords]\n",
        "    print(f\"Tokenized Sentences:\\n{tokens}\\n\")\n",
        "\n",
        "    return {\n",
        "        'sentences': sentences_no_stopwords,\n",
        "        'bigrams': bigrams,\n",
        "        'phrases': phrases,\n",
        "        'tokens': tokens\n",
        "    }\n",
        "\n",
        "\n",
        "html_data = \"\"\"\n",
        "<html>\n",
        "    <head>\n",
        "        <title>Sample HTML Document</title>\n",
        "    </head>\n",
        "    <body>\n",
        "        <h1>Welcome to NLP Processing</h1>\n",
        "        <p>This is a sample document with various HTML elements. It includes numbers like one hundred and twenty-three, and also special characters: @, #, $. Furthermore, let's test contractions such as won't, can't, and doesn't. Lastly, check out some common phrases: data science, machine learning, and artificial intelligence!</p>\n",
        "        <p>Another paragraph with different content. Numbers such as 2024, 15.6, and twenty-five should be handled. Also, more HTML tags and additional special characters like & and * should be removed.</p>\n",
        "    </body>\n",
        "</html>\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "preprocessed_data = preprocess_text(html_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-q4Jdca-4B-f",
        "outputId": "9a6101c0-3d7c-4691-88dc-290054a3b30f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text:\n",
            "\n",
            "<html>\n",
            "    <head>\n",
            "        <title>Sample HTML Document</title>\n",
            "    </head>\n",
            "    <body>\n",
            "        <h1>Welcome to NLP Processing</h1>\n",
            "        <p>This is a sample document with various HTML elements. It includes numbers like one hundred and twenty-three, and also special characters: @, #, $. Furthermore, let's test contractions such as won't, can't, and doesn't. Lastly, check out some common phrases: data science, machine learning, and artificial intelligence!</p>\n",
            "        <p>Another paragraph with different content. Numbers such as 2024, 15.6, and twenty-five should be handled. Also, more HTML tags and additional special characters like & and * should be removed.</p>\n",
            "    </body>\n",
            "</html>\n",
            "\n",
            "\n",
            "Text after removing HTML tags:\n",
            "\n",
            "\n",
            "\n",
            "Sample HTML Document\n",
            "\n",
            "\n",
            "Welcome to NLP Processing\n",
            "This is a sample document with various HTML elements. It includes numbers like one hundred and twenty-three, and also special characters: @, #, $. Furthermore, let's test contractions such as won't, can't, and doesn't. Lastly, check out some common phrases: data science, machine learning, and artificial intelligence!\n",
            "Another paragraph with different content. Numbers such as 2024, 15.6, and twenty-five should be handled. Also, more HTML tags and additional special characters like & and * should be removed.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Text after removing extra whitespaces:\n",
            "Sample HTML Document Welcome to NLP Processing This is a sample document with various HTML elements. It includes numbers like one hundred and twenty-three, and also special characters: @, #, $. Furthermore, let's test contractions such as won't, can't, and doesn't. Lastly, check out some common phrases: data science, machine learning, and artificial intelligence! Another paragraph with different content. Numbers such as 2024, 15.6, and twenty-five should be handled. Also, more HTML tags and additional special characters like & and * should be removed.\n",
            "\n",
            "Sentences after boundary detection:\n",
            "['Sample HTML Document Welcome to NLP Processing This is a sample document with various HTML elements.', 'It includes numbers like one hundred and twenty-three, and also special characters: @, #, $.', \"Furthermore, let's test contractions such as won't, can't, and doesn't.\", 'Lastly, check out some common phrases: data science, machine learning, and artificial intelligence!', 'Another paragraph with different content.', 'Numbers such as 2024, 15.6, and twenty-five should be handled.', 'Also, more HTML tags and additional special characters like & and * should be removed.']\n",
            "\n",
            "Sentences after expanding contractions:\n",
            "['Sample HTML Document Welcome to NLP Processing This is a sample document with various HTML elements.', 'It includes numbers like one hundred and twenty-three, and also special characters: @, #, $.', 'Furthermore, let us test contractions such as will not, cannot, and does not.', 'Lastly, check out some common phrases: data science, machine learning, and artificial intelligence!', 'Another paragraph with different content.', 'Numbers such as 2024, 15.6, and twenty-five should be handled.', 'Also, more HTML tags and additional special characters like & and * should be removed.']\n",
            "\n",
            "Sentences after removing special characters:\n",
            "['Sample HTML Document Welcome to NLP Processing This is a sample document with various HTML elements', 'It includes numbers like one hundred and twentythree and also special characters   ', 'Furthermore let us test contractions such as will not cannot and does not', 'Lastly check out some common phrases data science machine learning and artificial intelligence', 'Another paragraph with different content', 'Numbers such as 2024 156 and twentyfive should be handled', 'Also more HTML tags and additional special characters like  and  should be removed']\n",
            "\n",
            "Sentences after converting to lowercase:\n",
            "['sample html document welcome to nlp processing this is a sample document with various html elements', 'it includes numbers like one hundred and twentythree and also special characters   ', 'furthermore let us test contractions such as will not cannot and does not', 'lastly check out some common phrases data science machine learning and artificial intelligence', 'another paragraph with different content', 'numbers such as 2024 156 and twentyfive should be handled', 'also more html tags and additional special characters like  and  should be removed']\n",
            "\n",
            "Sentences after converting number words to numeric form:\n",
            "['sample html document welcome to nlp processing this is a sample document with various html elements', 'it includes numbers like 1 100 and twentythree and also special characters', 'furthermore let us test contractions such as will not cannot and does not', 'lastly check out some common phrases data science machine learning and artificial intelligence', 'another paragraph with different content', 'numbers such as 2024 156 and twentyfive should be handled', 'also more html tags and additional special characters like and should be removed']\n",
            "\n",
            "Sentences after removing numbers:\n",
            "['sample html document welcome to nlp processing this is a sample document with various html elements', 'it includes numbers like   and twentythree and also special characters', 'furthermore let us test contractions such as will not cannot and does not', 'lastly check out some common phrases data science machine learning and artificial intelligence', 'another paragraph with different content', 'numbers such as   and twentyfive should be handled', 'also more html tags and additional special characters like and should be removed']\n",
            "\n",
            "Sentences after removing stopwords:\n",
            "['sample html document welcome nlp processing sample document various html elements', 'includes numbers like twentythree also special characters', 'furthermore let us test contractions cannot', 'lastly check common phrases data science machine learning artificial intelligence', 'another paragraph different content', 'numbers twentyfive handled', 'also html tags additional special characters like removed']\n",
            "\n",
            "Extracted Bigrams:\n",
            "[]\n",
            "\n",
            "Extracted Phrases:\n",
            "['artificial intelligence', 'twentythree', 'science machine', 'another paragraph', 'different content', 'sample html document', 'welcome nlp processing sample document']\n",
            "\n",
            "Tokenized Sentences:\n",
            "[['sample', 'html', 'document', 'welcome', 'nlp', 'processing', 'sample', 'document', 'various', 'html', 'elements'], ['includes', 'numbers', 'like', 'twentythree', 'also', 'special', 'characters'], ['furthermore', 'let', 'us', 'test', 'contractions', 'can', 'not'], ['lastly', 'check', 'common', 'phrases', 'data', 'science', 'machine', 'learning', 'artificial', 'intelligence'], ['another', 'paragraph', 'different', 'content'], ['numbers', 'twentyfive', 'handled'], ['also', 'html', 'tags', 'additional', 'special', 'characters', 'like', 'removed']]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dwx20DKr5FMw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}